{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Potential Applications:** classify articles by topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From: http://brandonrose.org/clustering\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidv/anaconda/lib/python3.5/site-packages/IPython/core/formatters.py:92: DeprecationWarning: DisplayFormatter._ipython_display_formatter_default is deprecated: use @default decorator instead.\n",
      "  def _ipython_display_formatter_default(self):\n",
      "/Users/davidv/anaconda/lib/python3.5/site-packages/IPython/core/formatters.py:98: DeprecationWarning: DisplayFormatter._formatters_default is deprecated: use @default decorator instead.\n",
      "  def _formatters_default(self):\n",
      "/Users/davidv/anaconda/lib/python3.5/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n",
      "/Users/davidv/anaconda/lib/python3.5/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/Users/davidv/anaconda/lib/python3.5/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/Users/davidv/anaconda/lib/python3.5/site-packages/IPython/core/formatters.py:669: DeprecationWarning: PlainTextFormatter._singleton_printers_default is deprecated: use @default decorator instead.\n",
      "  def _singleton_printers_default(self):\n",
      "/Users/davidv/anaconda/lib/python3.5/site-packages/IPython/core/formatters.py:672: DeprecationWarning: PlainTextFormatter._type_printers_default is deprecated: use @default decorator instead.\n",
      "  def _type_printers_default(self):\n",
      "/Users/davidv/anaconda/lib/python3.5/site-packages/IPython/core/formatters.py:677: DeprecationWarning: PlainTextFormatter._deferred_printers_default is deprecated: use @default decorator instead.\n",
      "  def _deferred_printers_default(self):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Capitalism',\n",
       " 'Competition',\n",
       " 'Property Rights',\n",
       " 'Entrepreneurship',\n",
       " 'Free Markets',\n",
       " 'Market Process',\n",
       " 'Biographies',\n",
       " 'Innovation',\n",
       " 'Arts and Music',\n",
       " 'Technology']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_pickle('ArticleMetadata.pkl')\n",
    "\n",
    "articles.DatePublished = pd.to_datetime(articles.DatePublished)\n",
    "articles.Tags = articles.Tags.map(lambda x: str(x))\n",
    "articles.TagArray = articles.Tags.map(lambda x: x.split(','))\n",
    "articles.TagArray[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArticleId\n",
       "12897    \"AMC's Halt and Catch Fire is a brilliant achi...\n",
       "Name: FullText, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head(1).FullText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords, stemming, and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here I define a tokenizer and stemmer which returns the set of stems in the text that it is passed\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use extend so it's a big flat list of vocab\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in articles.FullText:\n",
    "    allwords_stemmed = tokenize_and_stem(i) #for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) #extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'current',\n",
       " 'american',\n",
       " 'elect',\n",
       " 'has',\n",
       " 'me',\n",
       " 'think',\n",
       " 'a',\n",
       " 'lot',\n",
       " 'about',\n",
       " '17th',\n",
       " 'centuri',\n",
       " 'england',\n",
       " 'the',\n",
       " '1650s',\n",
       " 'were',\n",
       " 'a',\n",
       " 'time',\n",
       " 'of',\n",
       " 'unpreced',\n",
       " 'upheav',\n",
       " 'in',\n",
       " 'england',\n",
       " 'the',\n",
       " 'decad',\n",
       " 'was',\n",
       " 'usher',\n",
       " 'in',\n",
       " 'with',\n",
       " 'the',\n",
       " 'public',\n",
       " 'trial',\n",
       " 'and',\n",
       " 'execut',\n",
       " 'of',\n",
       " 'the',\n",
       " 'king',\n",
       " 'and',\n",
       " 'the',\n",
       " 'creation',\n",
       " 'of',\n",
       " 'an',\n",
       " 'entir',\n",
       " 'new',\n",
       " 'form',\n",
       " 'of',\n",
       " 'govern',\n",
       " 'vote',\n",
       " 'in',\n",
       " 'liter',\n",
       " 'at',\n",
       " 'gunpoint',\n",
       " 'the',\n",
       " 'aftermath',\n",
       " 'of',\n",
       " 'a',\n",
       " 'long',\n",
       " 'and',\n",
       " 'bloodi',\n",
       " 'civil',\n",
       " 'war',\n",
       " 'left',\n",
       " 'the',\n",
       " 'nation',\n",
       " \"'s\",\n",
       " 'polit',\n",
       " 'and',\n",
       " 'religion',\n",
       " 'uncertain',\n",
       " 'and',\n",
       " 'the',\n",
       " 'nation',\n",
       " \"'s\",\n",
       " 'peopl',\n",
       " 'divid',\n",
       " 'the',\n",
       " 'poet',\n",
       " 'robert',\n",
       " 'herrick',\n",
       " 'right',\n",
       " 'said',\n",
       " 'of',\n",
       " 'the',\n",
       " 'time',\n",
       " 'that',\n",
       " 'sick',\n",
       " 'is',\n",
       " 'the',\n",
       " 'land',\n",
       " 'to',\n",
       " 'th',\n",
       " 'heart',\n",
       " 'and',\n",
       " 'doth',\n",
       " 'endur',\n",
       " 'more',\n",
       " 'danger',\n",
       " 'faint',\n",
       " 'by',\n",
       " 'her',\n",
       " \"desp'rat\",\n",
       " 'cure.Ã¢\\x80\\x9d',\n",
       " 'in',\n",
       " 'the',\n",
       " 'middl',\n",
       " 'of',\n",
       " 'all',\n",
       " 'the',\n",
       " 'upheav',\n",
       " 'a',\n",
       " 'book',\n",
       " 'about',\n",
       " 'fish',\n",
       " 'came',\n",
       " 'out.in',\n",
       " 'the',\n",
       " 'middl',\n",
       " 'of',\n",
       " 'all',\n",
       " 'this',\n",
       " 'in',\n",
       " 'an',\n",
       " 'odd',\n",
       " 'littl',\n",
       " 'book',\n",
       " 'was',\n",
       " 'publish',\n",
       " 'by',\n",
       " 'the',\n",
       " 'retir',\n",
       " 'ironmong',\n",
       " 'izaak',\n",
       " 'walton',\n",
       " 'titl',\n",
       " 'the',\n",
       " 'compleat',\n",
       " 'angler',\n",
       " 'and',\n",
       " 'frame',\n",
       " 'as',\n",
       " 'a',\n",
       " 'convers',\n",
       " 'among',\n",
       " 'three',\n",
       " 'friend',\n",
       " 'as',\n",
       " 'they',\n",
       " 'wander',\n",
       " 'through',\n",
       " 'the',\n",
       " 'english',\n",
       " 'countrysid',\n",
       " 'the',\n",
       " 'book',\n",
       " 'is',\n",
       " 'a',\n",
       " 'compendium',\n",
       " 'of',\n",
       " 'advic',\n",
       " 'about',\n",
       " 'fish',\n",
       " 'poem',\n",
       " 'about',\n",
       " 'fish',\n",
       " 'and',\n",
       " 'philosophi',\n",
       " 'about',\n",
       " 'fish',\n",
       " 'it',\n",
       " 'was',\n",
       " 'wild',\n",
       " 'popular',\n",
       " 'the',\n",
       " 'compleat',\n",
       " 'angler',\n",
       " 'was',\n",
       " 'print',\n",
       " 'five',\n",
       " 'time',\n",
       " 'in',\n",
       " 'the',\n",
       " '17th',\n",
       " 'centuri',\n",
       " 'ten',\n",
       " 'time',\n",
       " 'in',\n",
       " 'the',\n",
       " '18th',\n",
       " 'over',\n",
       " 'time',\n",
       " 'in',\n",
       " 'the',\n",
       " '19th',\n",
       " 'and',\n",
       " 'over',\n",
       " 'time',\n",
       " 'in',\n",
       " 'the',\n",
       " '20th',\n",
       " 'centuri',\n",
       " 'and',\n",
       " 'it',\n",
       " 'is',\n",
       " 'still',\n",
       " 'in',\n",
       " 'print',\n",
       " 'today',\n",
       " 'whi',\n",
       " 'in',\n",
       " 'a',\n",
       " 'world',\n",
       " 'turn',\n",
       " 'upsid',\n",
       " 'down',\n",
       " 'was',\n",
       " 'anyon',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'write',\n",
       " 'or',\n",
       " 'read',\n",
       " 'about',\n",
       " 'fish',\n",
       " 'whi',\n",
       " 'was',\n",
       " 'appar',\n",
       " 'everyon',\n",
       " 'interest',\n",
       " 'go',\n",
       " 'fish',\n",
       " 'the',\n",
       " 'polit',\n",
       " 'and',\n",
       " 'religi',\n",
       " 'world',\n",
       " 'of',\n",
       " 'england',\n",
       " 'in',\n",
       " 'the',\n",
       " '1650s',\n",
       " 'had',\n",
       " 'becom',\n",
       " 'so',\n",
       " 'contenti',\n",
       " 'and',\n",
       " 'so',\n",
       " 'divis',\n",
       " 'so',\n",
       " 'fraught',\n",
       " 'with',\n",
       " 'violenc',\n",
       " 'and',\n",
       " 'with',\n",
       " 'danger',\n",
       " 'that',\n",
       " 'the',\n",
       " 'onli',\n",
       " 'respons',\n",
       " 'for',\n",
       " 'a',\n",
       " 'sensibl',\n",
       " 'person',\n",
       " 'was',\n",
       " 'to',\n",
       " 'separ',\n",
       " 'from',\n",
       " 'it',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'possibl',\n",
       " 'walton',\n",
       " \"'s\",\n",
       " 'book',\n",
       " 'is',\n",
       " 'a',\n",
       " 'peac',\n",
       " 'yet',\n",
       " 'radic',\n",
       " 'respons',\n",
       " 'to',\n",
       " 'the',\n",
       " 'horror',\n",
       " 'of',\n",
       " 'contemporari',\n",
       " 'polit',\n",
       " 'he',\n",
       " 'and',\n",
       " 'other',\n",
       " 'writer',\n",
       " 'like',\n",
       " 'him',\n",
       " 'chose',\n",
       " 'to',\n",
       " 'leav',\n",
       " 'london',\n",
       " 'retir',\n",
       " 'to',\n",
       " 'the',\n",
       " 'countri',\n",
       " 'and',\n",
       " 'spend',\n",
       " 'their',\n",
       " 'time',\n",
       " 'in',\n",
       " 'pleasant',\n",
       " 'and',\n",
       " 'medit',\n",
       " 'pursuit',\n",
       " 'the',\n",
       " 'world',\n",
       " 'had',\n",
       " 'becom',\n",
       " 'so',\n",
       " 'divis',\n",
       " 'and',\n",
       " 'violent',\n",
       " 'that',\n",
       " 'the',\n",
       " 'onli',\n",
       " 'sensibl',\n",
       " 'respons',\n",
       " 'was',\n",
       " 'to',\n",
       " 'separ',\n",
       " 'from',\n",
       " 'it',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'possible.it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'easi',\n",
       " 'to',\n",
       " 'think',\n",
       " 'of',\n",
       " 'such',\n",
       " 'a',\n",
       " 'respons',\n",
       " 'as',\n",
       " 'a',\n",
       " 'craven',\n",
       " 'retreat',\n",
       " 'but',\n",
       " 'i',\n",
       " 'think',\n",
       " 'that',\n",
       " 'what',\n",
       " 'walton',\n",
       " 'is',\n",
       " 'do',\n",
       " 'is',\n",
       " 'insist',\n",
       " 'on',\n",
       " 'the',\n",
       " 'continu',\n",
       " 'of',\n",
       " 'a',\n",
       " 'world',\n",
       " 'that',\n",
       " 'is',\n",
       " 'beyond',\n",
       " 'the',\n",
       " 'reach',\n",
       " 'of',\n",
       " 'polit',\n",
       " 'one',\n",
       " 'that',\n",
       " 'is',\n",
       " 'about',\n",
       " 'the',\n",
       " 'person',\n",
       " 'and',\n",
       " 'the',\n",
       " 'peac',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'the',\n",
       " 'public',\n",
       " 'and',\n",
       " 'polit',\n",
       " 'that',\n",
       " 'privat',\n",
       " 'world',\n",
       " 'is',\n",
       " 'import',\n",
       " 'and',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'protect',\n",
       " 'and',\n",
       " 'valu',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'sacrif',\n",
       " 'for',\n",
       " 'the',\n",
       " 'greater',\n",
       " 'concernsÃ¢\\x80\\x9d',\n",
       " 'of',\n",
       " 'nation',\n",
       " 'polit',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'heat',\n",
       " 'polit',\n",
       " 'rhetor',\n",
       " 'and',\n",
       " 'theolog',\n",
       " 'debat',\n",
       " 'the',\n",
       " 'compleat',\n",
       " 'angler',\n",
       " 'focus',\n",
       " 'on',\n",
       " 'quiet',\n",
       " 'and',\n",
       " 'friend',\n",
       " 'convers',\n",
       " 'among',\n",
       " 'friend',\n",
       " 'the',\n",
       " 'deepest',\n",
       " 'divis',\n",
       " 'of',\n",
       " 'opinion',\n",
       " 'are',\n",
       " 'over',\n",
       " 'fly-cast',\n",
       " 'versus',\n",
       " 'bait',\n",
       " 'fish',\n",
       " 'and',\n",
       " 'in',\n",
       " 'contrast',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bloodbath',\n",
       " 'of',\n",
       " 'the',\n",
       " 'civil',\n",
       " 'war',\n",
       " 'the',\n",
       " 'onli',\n",
       " 'one',\n",
       " 'harm',\n",
       " 'in',\n",
       " 'walton',\n",
       " \"'s\",\n",
       " 'book',\n",
       " 'are',\n",
       " 'fish',\n",
       " 'and',\n",
       " 'bait',\n",
       " 'anim',\n",
       " 'even',\n",
       " 'the',\n",
       " 'worm',\n",
       " 'and',\n",
       " 'frog',\n",
       " 'who',\n",
       " 'are',\n",
       " 'to',\n",
       " 'be',\n",
       " 'use',\n",
       " 'as',\n",
       " 'bait',\n",
       " 'must',\n",
       " 'be',\n",
       " 'use',\n",
       " 'he',\n",
       " 'say',\n",
       " 'as',\n",
       " 'though',\n",
       " 'you',\n",
       " 'love',\n",
       " 'him',\n",
       " 'that',\n",
       " 'is',\n",
       " 'harm',\n",
       " 'him',\n",
       " 'as',\n",
       " 'littl',\n",
       " 'as',\n",
       " 'you',\n",
       " 'may',\n",
       " 'possibl',\n",
       " 'that',\n",
       " 'he',\n",
       " 'may',\n",
       " 'live',\n",
       " 'the',\n",
       " 'longer.Ã¢\\x80\\x9d',\n",
       " 'this',\n",
       " 'is',\n",
       " 'no',\n",
       " 'accid',\n",
       " 'walton',\n",
       " 'draw',\n",
       " 'attent',\n",
       " 'to',\n",
       " 'the',\n",
       " 'contrast',\n",
       " 'between',\n",
       " 'the',\n",
       " 'life',\n",
       " 'he',\n",
       " 'has',\n",
       " 'chosen',\n",
       " 'in',\n",
       " 'the',\n",
       " 'countri',\n",
       " 'and',\n",
       " 'the',\n",
       " 'live',\n",
       " 'of',\n",
       " 'those',\n",
       " 'who',\n",
       " 'stay',\n",
       " 'in',\n",
       " 'london',\n",
       " 'no',\n",
       " 'life',\n",
       " 'my',\n",
       " 'honest',\n",
       " 'scholar',\n",
       " 'no',\n",
       " 'life',\n",
       " 'so',\n",
       " 'happi',\n",
       " 'and',\n",
       " 'so',\n",
       " 'pleasant',\n",
       " 'as',\n",
       " 'the',\n",
       " 'life',\n",
       " 'of',\n",
       " 'a',\n",
       " 'well',\n",
       " 'govern',\n",
       " 'angler',\n",
       " 'for',\n",
       " 'when',\n",
       " 'the',\n",
       " 'lawyer',\n",
       " 'is',\n",
       " 'swallow',\n",
       " 'up',\n",
       " 'with',\n",
       " 'busi',\n",
       " 'and',\n",
       " 'the',\n",
       " 'statesman',\n",
       " 'is',\n",
       " 'prevent',\n",
       " 'or',\n",
       " 'contriv',\n",
       " 'plot',\n",
       " 'then',\n",
       " 'we',\n",
       " 'sit',\n",
       " 'on',\n",
       " 'cowslip-bank',\n",
       " 'hear',\n",
       " 'the',\n",
       " 'bird',\n",
       " 'sing',\n",
       " 'and',\n",
       " 'possess',\n",
       " 'ourselv',\n",
       " 'in',\n",
       " 'as',\n",
       " 'much',\n",
       " 'quietness',\n",
       " 'as',\n",
       " 'these',\n",
       " 'silent',\n",
       " 'silver',\n",
       " 'stream',\n",
       " 'which',\n",
       " 'we',\n",
       " 'now',\n",
       " 'see',\n",
       " 'glide',\n",
       " 'so',\n",
       " 'quiet',\n",
       " 'by',\n",
       " 'us',\n",
       " 'inde',\n",
       " 'my',\n",
       " 'good',\n",
       " 'scholar',\n",
       " 'we',\n",
       " 'may',\n",
       " 'say',\n",
       " 'of',\n",
       " 'angl',\n",
       " 'as',\n",
       " 'dr.',\n",
       " 'botel',\n",
       " 'said',\n",
       " 'of',\n",
       " 'strawberri',\n",
       " 'doubtless',\n",
       " 'god',\n",
       " 'could',\n",
       " 'have',\n",
       " 'made',\n",
       " 'a',\n",
       " 'better',\n",
       " 'berri',\n",
       " 'but',\n",
       " 'doubtless',\n",
       " 'god',\n",
       " 'never',\n",
       " 'did',\n",
       " 'and',\n",
       " 'so',\n",
       " 'if',\n",
       " 'i',\n",
       " 'might',\n",
       " 'be',\n",
       " 'judg',\n",
       " 'god',\n",
       " 'never',\n",
       " 'did',\n",
       " 'make',\n",
       " 'a',\n",
       " 'more',\n",
       " 'calm',\n",
       " 'quiet',\n",
       " 'innoc',\n",
       " 'recreat',\n",
       " 'than',\n",
       " 'angl',\n",
       " 'to',\n",
       " 'be',\n",
       " 'free',\n",
       " 'to',\n",
       " 'fish',\n",
       " 'for',\n",
       " 'walton',\n",
       " 'is',\n",
       " 'to',\n",
       " 'be',\n",
       " 'free',\n",
       " 'to',\n",
       " 'live',\n",
       " 'in',\n",
       " 'peac',\n",
       " 'and',\n",
       " 'free',\n",
       " 'to',\n",
       " 'talk',\n",
       " 'about',\n",
       " 'god',\n",
       " 'or',\n",
       " 'poetri',\n",
       " 'or',\n",
       " 'philosophi',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'about',\n",
       " 'transient',\n",
       " 'and',\n",
       " 'contenti',\n",
       " 'contemporari',\n",
       " 'concern',\n",
       " 'it',\n",
       " 'is',\n",
       " 'to',\n",
       " 'choos',\n",
       " 'innoc',\n",
       " 'as',\n",
       " 'much',\n",
       " 'as',\n",
       " 'possibl',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'the',\n",
       " 'guilt',\n",
       " 'of',\n",
       " 'choos',\n",
       " 'side',\n",
       " 'in',\n",
       " 'polit',\n",
       " 'and',\n",
       " 'religi',\n",
       " 'contest',\n",
       " 'with',\n",
       " 'no',\n",
       " 'good',\n",
       " 'altern',\n",
       " 'do',\n",
       " 'peac',\n",
       " 'thing',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'achiev',\n",
       " 'great',\n",
       " 'say',\n",
       " 'walton',\n",
       " 'he',\n",
       " 'would',\n",
       " 'rather',\n",
       " 'prove',\n",
       " 'myself',\n",
       " 'a',\n",
       " 'gentleman',\n",
       " 'by',\n",
       " 'be',\n",
       " 'learn',\n",
       " 'and',\n",
       " 'humbl',\n",
       " 'valiant',\n",
       " 'and',\n",
       " 'inoffens',\n",
       " 'virtuous',\n",
       " 'and',\n",
       " 'communicable.Ã¢\\x80\\x9d',\n",
       " 'in',\n",
       " 'other',\n",
       " 'word',\n",
       " 'by',\n",
       " 'do',\n",
       " 'anyth',\n",
       " 'peac',\n",
       " 'to',\n",
       " 'be',\n",
       " 'free',\n",
       " 'is',\n",
       " 'to',\n",
       " 'choos',\n",
       " 'innocence.it',\n",
       " \"'s\",\n",
       " 'no',\n",
       " 'surpris',\n",
       " 'then',\n",
       " 'that',\n",
       " 'the',\n",
       " '17th',\n",
       " 'centuri',\n",
       " 'and',\n",
       " 'walton',\n",
       " 'have',\n",
       " 'been',\n",
       " 'so',\n",
       " 'much',\n",
       " 'on',\n",
       " 'my',\n",
       " 'mind',\n",
       " 'of',\n",
       " 'late',\n",
       " 'as',\n",
       " 'the',\n",
       " 'elect',\n",
       " 'continu',\n",
       " 'to',\n",
       " 'heat',\n",
       " 'up',\n",
       " 'as',\n",
       " 'divis',\n",
       " 'among',\n",
       " 'parti',\n",
       " 'grow',\n",
       " 'deeper',\n",
       " 'and',\n",
       " 'more',\n",
       " 'intract',\n",
       " 'and',\n",
       " 'as',\n",
       " 'the',\n",
       " 'rhetor',\n",
       " 'of',\n",
       " 'candid',\n",
       " 'and',\n",
       " 'their',\n",
       " 'support',\n",
       " 'becom',\n",
       " 'more',\n",
       " 'vicious',\n",
       " 'everi',\n",
       " 'day',\n",
       " 'it',\n",
       " 'becom',\n",
       " 'harder',\n",
       " 'and',\n",
       " 'harder',\n",
       " 'to',\n",
       " 'insist',\n",
       " 'on',\n",
       " 'the',\n",
       " 'valu',\n",
       " 'and',\n",
       " 'the',\n",
       " 'import',\n",
       " 'of',\n",
       " 'activ',\n",
       " 'discuss',\n",
       " 'and',\n",
       " 'thought',\n",
       " 'that',\n",
       " 'are',\n",
       " 'not',\n",
       " 'polit',\n",
       " 'it',\n",
       " 'becom',\n",
       " 'harder',\n",
       " 'but',\n",
       " 'it',\n",
       " 'becom',\n",
       " 'more',\n",
       " 'import',\n",
       " 'i',\n",
       " 'am',\n",
       " 'of',\n",
       " 'the',\n",
       " 'mind',\n",
       " 'late',\n",
       " 'that',\n",
       " 'the',\n",
       " 'most',\n",
       " 'radic',\n",
       " 'thing',\n",
       " 'that',\n",
       " 'anyon',\n",
       " 'can',\n",
       " 'do',\n",
       " 'in',\n",
       " 'this',\n",
       " 'elect',\n",
       " 'season',\n",
       " 'is',\n",
       " 'to',\n",
       " 'watch',\n",
       " 'basebal',\n",
       " 'hang',\n",
       " 'out',\n",
       " 'with',\n",
       " 'friend',\n",
       " 'make',\n",
       " 'jam',\n",
       " 'read',\n",
       " 'to',\n",
       " 'a',\n",
       " 'child',\n",
       " 'or',\n",
       " 'go',\n",
       " 'fish',\n",
       " 'insist',\n",
       " 'on',\n",
       " 'the',\n",
       " 'import',\n",
       " 'of',\n",
       " 'other',\n",
       " 'thing',\n",
       " 'for',\n",
       " 'yourself',\n",
       " 'and',\n",
       " 'remind',\n",
       " 'other',\n",
       " 'of',\n",
       " 'their',\n",
       " 'import',\n",
       " 'as',\n",
       " 'well',\n",
       " 'let',\n",
       " 'the',\n",
       " 'politician',\n",
       " 'and',\n",
       " 'the',\n",
       " 'pundit',\n",
       " 'storm',\n",
       " 'and',\n",
       " 'howl',\n",
       " 'sure',\n",
       " 'we',\n",
       " 'have',\n",
       " 'better',\n",
       " 'thing',\n",
       " 'to',\n",
       " 'do',\n",
       " 'let',\n",
       " 'us',\n",
       " 'say',\n",
       " 'with',\n",
       " 'walton',\n",
       " 'whenev',\n",
       " 'we',\n",
       " 'can',\n",
       " 'i',\n",
       " 'have',\n",
       " 'laid',\n",
       " 'asid',\n",
       " 'my',\n",
       " 'busi',\n",
       " 'and',\n",
       " 'gone',\n",
       " \"a'fishing.Ã¢\\x80\\x9d\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 19239198 items in vocab_frame\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>amc</th>\n",
       "      <td>amc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'s</th>\n",
       "      <td>'s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>halt</th>\n",
       "      <td>halt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catch</th>\n",
       "      <td>catch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fire</th>\n",
       "      <td>fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brilliant</th>\n",
       "      <td>brilliant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>achiev</th>\n",
       "      <td>achievement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>show</th>\n",
       "      <td>show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vibrant</th>\n",
       "      <td>vibrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look</th>\n",
       "      <td>look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>at</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emerg</th>\n",
       "      <td>emerging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person</th>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comput</th>\n",
       "      <td>computer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>industri</th>\n",
       "      <td>industry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>earli</th>\n",
       "      <td>early</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980s</th>\n",
       "      <td>1980s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>but</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>more</th>\n",
       "      <td>more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>than</th>\n",
       "      <td>than</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 words\n",
       "amc                amc\n",
       "'s                  's\n",
       "halt              halt\n",
       "and                and\n",
       "catch            catch\n",
       "fire              fire\n",
       "is                  is\n",
       "a                    a\n",
       "brilliant    brilliant\n",
       "achiev     achievement\n",
       "the                the\n",
       "show              show\n",
       "is                  is\n",
       "a                    a\n",
       "vibrant        vibrant\n",
       "look              look\n",
       "at                  at\n",
       "the                the\n",
       "emerg         emerging\n",
       "person        personal\n",
       "comput        computer\n",
       "industri      industry\n",
       "in                  in\n",
       "the                the\n",
       "earli            early\n",
       "1980s            1980s\n",
       "but                but\n",
       "more              more\n",
       "than              than\n",
       "that              that"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_frame.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf and document similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 14s, sys: 9.03 s, total: 12min 23s\n",
      "Wall time: 12min 37s\n",
      "(13835, 399)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(articles.FullText) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function print>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print\n",
    "print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "\n",
    "%time km.fit(tfidf_matrix)\n",
    "\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "#uncomment the below to save your model \n",
    "#since I've already run my model I am loading from the pickle\n",
    "\n",
    "#joblib.dump(km,  'doc_cluster.pkl')\n",
    "\n",
    "km = joblib.load('doc_cluster.pkl')\n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters, 'genre': genres }\n",
    "\n",
    "frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster', 'genre'])\n",
    "\n",
    "frame['cluster'].value_counts() #number of films per cluster (clusters from 0 to 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = frame['rank'].groupby(frame['cluster']) #groupby cluster for aggregation purposes\n",
    "\n",
    "grouped.mean() #average rank (1 to 100) per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: #replace 6 with n words per cluster\n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "    print(\"Cluster %d titles:\" % i, end='')\n",
    "    for title in frame.ix[i]['title'].values.tolist():\n",
    "        print(' %s,' % title, end='')\n",
    "    print() #add whitespace\n",
    "    print() #add whitespace\n",
    "    \n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
